---
title: "Heart Disease Analysis"
author: "German Batuista (gbauti5@illinois.edu)"
date: "05/04/2021"
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: true
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center')
```

```{r, load-packages, include = FALSE}
# load packages
library(caret)
library(cvms)
```

```{r read-full-data, warning = FALSE, message = FALSE}
# read full data
hd = readr::read_csv("data/hd.csv")
```

## Abstract

This analysis was interested in creating a model that detects the presence of heart disease using variables that came from non-invasive procedures and a patient's general health profile to treat heart disease as early as possible.
We trained both a random forest model and a logistic regression model with variables concerning patients' general health such as age, chest pain type, cholesterol, and more elaborate data such as maximum heart rate achieved and whether the patient experienced angina via exercise.
We found that our logistic regression model with primary variables (model II) and random forest model including secondary variables (model III) performed best at a cutoff of 0.30. Because model III did not significantly predict better than model II and model II was simpler, model II is our preferred model. Thus, primary variables: `age`, `sex`, `cp`, `trestbps`, `chol`, `fbs`, `restecg` sufficed for predicting heart disease sufficiently well in this analysis

## Introduction

Heart disease is the leading cause of death in the United States according to Centers for Disease and Prevention (CDC). The data set `data/hd.csv`, released in July 1988, contains patient health information (n = 920) with 15 variables from four locations: Cleveland Clinic Foundation; Hungarian Institute of Cardiology, Budapest; V.A. Medical Center, Long Beach, CA; and University Hospital, Zurich, Switzerland. This analysis was interested in creating a model that detects the presence of heart disease using variables that came from non-invasive procedures and a patient's general health profile to treat heart disease as early as possible. The model is intended to give an initial diagnosis for heart disease and not as an ultimate determinant. 

```{r}
set.seed(432)
# Percentage of original data set sampled for the train data set
train_percentage = 0.80
# Split the data
trn_idx = createDataPartition(hd$num, p = train_percentage)
na_hd_trn = hd[trn_idx$Resample1, ]
na_hd_tst = hd[-trn_idx$Resample1, ]

preprocess_data = function(data) {
  # Remove predictors slope, ca, thal
  # Deemed intrusive to gather or have a high proportion of NA values
  data = data[, -c(11, 12, 13)]
  
  # WARNING: Lots of cholesterol data has value 0
  data[which(data$chol == 0), ]$chol = NA
  
  # Remove any observations with NA values in their row
  data = na.omit(data)
  
  # Change character predictors to be factors
  data$sex = factor(data$sex, levels = c("0", "1"), labels = c("F", "M"))
  data$cp = factor(data$cp)
  data$fbs = factor(data$fbs, levels = c("0", "1"), labels = c("false", "true"))
  data$restecg = factor(data$restecg)
  data$exang = factor(data$exang, levels = c("0", "1"), labels = c("no", "yes"))
  data$location = factor(data$location)
  data$num = factor(data$num)

  # Create binary predictor with no: if num = v0 and yes: otherwise
  data$heart = factor(ifelse(data$num == "v0", "no", "yes"), levels = c("no", "yes"))
  
  # Return preprocessed data
  data
}
# Preprocess train and test data
hd_trn = preprocess_data(na_hd_trn)
hd_tst = preprocess_data(na_hd_tst)
```

*Note*: The variables `slope`, `thal`, and `ca` were removed from the train and test data set due to having significant missing data and `ca` requires an invasive procedure involving flourscopy. The binary variable `heart` was added to indicate the presence of heart disease. Any missing data was removed afterwards.

The following summary is for the training data (n = 529), `hd_trn`.
```{r}
# Summary of training data
skimr::skim(hd_trn)
```

Approximately 73% of patients were male while 27% were female. This may show that males disproportionately get heart disease compared to females assuming that females were accurately accounted for in the sampling procedure.
Approximately 27% of patients have some type of angina pain (severe chest pain).
Approximately, 47% of the patients have heart disease and 53% do not thus a useful heart disease diagnostic model should at least have an accuracy of 53%.
After data pre-processing, only patients in Cleveland (Ohio), Budapest (Hungary), and Long Beach (California) remained in the testing data with approximately 46% of patients located in Cleveland, 38% in Budapest, and 16% in Long Beach.
The patients' ages range from 28 years to 77 years with an average of 53.1 year. This is important to note as general health is dependent on age.

**Plot I**: chest pain type `cp` across age `age` and presence of heart disease
```{r}
# Plot of chest pain across age and heart disease presence
plot(as.numeric(cp) ~ age, data = hd_trn, pch = 20, col = hd_trn$heart,
     main = "Type of chest pain across age", ylab = "chest pain")
grid()
legend("bottomleft", c("No", "Yes"), title = "Heart Disease", col = hd_trn$heart, pch = 20, 
       inset = c(0, -0.33), xpd = TRUE, horiz = TRUE)
```
Based on plot I, most patients who had heart disease also had chest pain of type 4 regardless of age.
There appears to more patients with heart disease as age increases. Namely, 12% of patients were both less than 50 years old and had heart disease while 36% of patients both were older than 50 and had heart disease.
Chest pain appears to be an important variable for predicting heart disease.

**Plot II**: cholesterol `chol` across age `age` and presence of heart disease
```{r}
# Plot of cholesterol measure across age and heart disease presence
plot(chol ~ age, data = hd_trn, pch = 20, col = hd_trn$heart,
     main = "Measure of cholesterol across age", ylab = "cholesterol (mg/dl)")
abline(v = 45, lty = 2)
grid()
legend("bottomleft", c("No", "Yes"), title = "Heart Disease", col = hd_trn$heart, pch = 20,
       inset = c(0, -0.33), xpd = TRUE, horiz = TRUE)
```
Based on plot II, cholesterol is not a strong indicator of heart disease since patients older than 45 were diagnosed with heart disease significantly more often than patients younger than 45 regardless of cholesterol levels.
Age appears to be an important variable for predicting heart disease.

## Methods
### Data

The training data set `hd_trn` contains 16 variables which were categorized as:

- *primary*: ideal variables in our model.
- *secondary*: variables considered if primary variables did not suffice our model requirements (accuracy, sensitivity, specificity).
- *unconsidered*: variables not considered in our analysis.
- *response*: variables describing presence or condition of heart disease.

Primary variables

- `age`: age in years 
- `sex`: sex (1 = male; 0 = female)
- `cp`: chest pain type 
    - 1: typical angina 
    - 2: atypical angina 
    - 3: non-anginal pain 
    - 4: asymptomatic 
- `trestbps`: resting blood pressure (in mm Hg on admission to the hospital) 
- `chol`: serum cholesterol in mg/dl 
- `fbs`: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 
- `restecg`: resting electrocardiographic results 
    - 0: normal 
    - 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) 
    - 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 

Secondary variables

- `thalach`: maximum heart rate achieved 
- `exang`: exercise induced angina (1 = yes; 0 = no) 
- `oldpeak`: ST depression induced by exercise relative to rest 
- `slope`: the slope of the peak exercise ST segment (*) 
    - 1: upsloping 
    - 2: flat 
    - 3: downsloping 
- `thal`: 3 = normal; 6 = fixed defect; 7 = reversable defect (*)

Unconsidered variables

- `ca`: number of major vessels (0-3) colored by flourosopy (*)
- `location`: location patient was observed at (`ch`, `cl`, `hu`, `va`)

Response variables

- `num`: angiographic disease status i.e. the number of major heart vessels with greater than 50% diameter narrowing.
    - `v0`: 0 major vessels with greater than 50% diameter narrowing.
    - `v1`: 1 major vessels with greater than 50% diameter narrowing.
    - `v2`: 2 major vessels with greater than 50% diameter narrowing. 
    - `v3`: 3 major vessels with greater than 50% diameter narrowing.
    - `v4`: 4 major vessels with greater than 50% diameter narrowing.

- `heart`: binary variable describing presence of heart disease derived from `num`
    - `no` if `num` has value `v0` (negative class)
    - `yes` otherwise (positive class)
    
This analysis was primarily interested in detecting heart disease, therefore `heart` was used as the response variable while primary and secondary variables were potential predictors.

*Note*: The variables `slope`, `thal`, and `ca` were removed while the new variable `heart` was added in `hd_trn` and `hd_tst`. Then any missing data was removed.

### Source

The data stored in `data/hd.csv` was accessed through the UCI Machine Learning Repository.

- [Documentation: UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)

The data used in the creation of `data/hd.csv` was collected from the four following locations:

1. Cleveland Clinic Foundation
2. Hungarian Institute of Cardiology, Budapest
3. V.A. Medical Center, Long Beach, CA
4. University Hospital, Zurich, Switzerland

The contributors of the data have requested that any publications resulting from the use of the data include the  names of the principal investigator responsible for the data collection at each institution.  They are:

1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D. 

### Models

We considered a random forest model and a logistic regression model to detect `heart disease`.
First, we used five fold cross validation on both models which were fit using all primary variables. Their predictive performance were measured using accuracy, sensitivity (true positive rate), and specificity (true negative rate). To optimize predictive performance, cutoff values from 0 to 1 (inclusive) going up by 0.05 were considered. The cutoff values that produced high sensitivity (sensitivity > 0.80), high accuracy, and moderately high specificity were chosen to represent the optimal random forest model and logistic regression model respectively.

We repeated the exact same process as above, but we also added all secondary variables to see if predictive performance improved and if this was significant enough to add secondary variables to our previous models.

```{r}
# 5 fold cross validation 
set.seed(432)
cv_folds = trainControl(method = "cv", number = 5)

# Possible cutoff values for classification
cutoffs = seq(0, 1, by = 0.05)
```

```{r, rf}
# I) Fit random forest model with primary variables
forest_train = train(heart ~ age + sex + cp + trestbps + chol + fbs + restecg, 
      data = hd_trn,
      method = "rf",
      trControl = cv_folds,
      tuneLength = 9)

# Make predictions on testing data
forest_pred = predict(forest_train, hd_tst, type = "prob")[, "yes"]
forest_results = tibble::tibble(predicted = forest_pred, actual = hd_tst$heart)

# Function that outputs metrics based on cutoff value for random forest model
make_forest_eval = function(cutoff) {
  evaluate(data = forest_results,
         target_col = "actual",
         prediction_cols = "predicted",
         type = "binomial",
         cutoff = cutoff,
         positive = "yes")
}
# List of metrics for each cutoff value
forest_list = sapply(cutoffs, make_forest_eval)
```

```{r, logit}
# II) Fit logistic regression model with primary variables
logit_train = train(heart ~ age + sex + cp + trestbps + chol + fbs + restecg, 
      data = hd_trn,
      method = "glm",
      trControl = cv_folds)

# Make predictions on testing data
logit_pred = predict(logit_train, hd_tst, type = "prob")[, "yes"]
logit_results = tibble::tibble(predicted = logit_pred, actual = hd_tst$heart)

# Function that outputs metrics based on cutoff value for logit model
make_logit_eval = function(cutoff) {
  evaluate(data = logit_results,
         target_col = "actual",
         prediction_cols = "predicted",
         type = "binomial",
         cutoff = cutoff,
         positive = "yes")
}
# List of metrics for each cutoff value
logit_list = sapply(cutoffs, make_logit_eval)
```

```{r, full_rf}
# III) Fit random forest model including secondary variables
full_forest_train = train(heart ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak, 
      data = hd_trn,
      method = "rf",
      trControl = cv_folds,
      tuneLength = 9)

# Make predictions on testing data
full_forest_pred = predict(full_forest_train, hd_tst, type = "prob")[, "yes"]
full_forest_results = tibble::tibble(predicted = full_forest_pred, actual = hd_tst$heart)

# Function that outputs metrics based on cutoff value for random forest model
make_full_forest_eval = function(cutoff) {
  evaluate(data = full_forest_results,
         target_col = "actual",
         prediction_cols = "predicted",
         type = "binomial",
         cutoff = cutoff,
         positive = "yes")
}
# List of metrics for each cutoff value
full_forest_list = sapply(cutoffs, make_full_forest_eval)
```

```{r, full_logit}
# IV) Fit logistic regression model including secondary variables
full_logit_train = train(heart ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak, 
      data = hd_trn,
      method = "glm",
      trControl = cv_folds)

# Make predictions on testing data
full_logit_pred = predict(full_logit_train, hd_tst, type = "prob")[, "yes"]
full_logit_results = tibble::tibble(predicted = full_logit_pred, actual = hd_tst$heart)

# Function that outputs metrics based on cutoff value for logit model
make_full_logit_eval = function(cutoff) {
  evaluate(data = full_logit_results,
         target_col = "actual",
         prediction_cols = "predicted",
         type = "binomial",
         cutoff = cutoff,
         positive = "yes")
}
# List of metrics for each cutoff value
full_logit_list = sapply(cutoffs, make_full_logit_eval)
```

## Results

Here we plotted the sensitivity, specificity, and accuracy at cutoffs from 0 to 1 (inclusive) up by 0.05 for four models total.

I) Random forest model fitted with all primary variables.
```{r}
# Sensitivity, specificity and accuracy plot for random forest model with primary variables
plot(cutoffs, forest_list["Sensitivity", ], col = "darkorange", pch = 20, type = "b",
     ylab = "Performance", main = "Metrics for Random Forest Model")
points(cutoffs, forest_list["Specificity", ], col = "midnightblue", pch = 20, type = "b")
points(cutoffs, forest_list["Accuracy", ], col = "mediumseagreen", pch = 20, type = "b")
abline(v = 0.35, lty = 2)
legend("bottomleft", c("Sensitivity", "Specificity", "Accuracy"), col = c("darkorange", "midnightblue", "mediumseagreen"), pch = 20,
       inset = c(-0.14, -0.33), xpd = TRUE, horiz = TRUE)
```

II) Logistic regression model fitted with all primary variables.
```{r}
# Sensitivity, specificity and accuracy plot for logit model with primary variables
plot(cutoffs, logit_list["Sensitivity", ], col = "purple", pch = 20, type = "b",
     ylab = "Performance", main = "Metrics for Logit Model")
points(cutoffs, logit_list["Specificity", ], col = "firebrick", pch = 20, type = "b")
points(cutoffs, logit_list["Accuracy", ], col = "chartreuse", pch = 20, type = "b")
abline(v = 0.35, lty = 2)
legend("bottomleft", c("Sensitivity", "Specificity", "Accuracy"), col = c("purple", "firebrick", "chartreuse"), pch = 20,
       inset = c(-0.14, -0.33), xpd = TRUE, horiz = TRUE)
```

III) Random forest model fitted with all primary variables including secondary variables.
```{r}
# Sensitivity, specificity and accuracy plot for random forest model including secondary variables
plot(cutoffs, full_forest_list["Sensitivity", ], col = "darkorange", pch = 20, type = "b", lty = 2,
     ylab = "Performance", main = "Metrics for Full Random Forest Model")
points(cutoffs, full_forest_list["Specificity", ], col = "midnightblue", pch = 20, type = "b", lty = 2)
points(cutoffs, full_forest_list["Accuracy", ], col = "mediumseagreen", pch = 20, type = "b", lty = 2)
abline(v = 0.35, lty = 2)
legend("bottomleft", c("Sensitivity", "Specificity", "Accuracy"), col = c("darkorange", "midnightblue", "mediumseagreen"), pch = 20,
       inset = c(-0.14, -0.33), xpd = TRUE, horiz = TRUE)
```

IV) Logistic regression model fitted with all primary variables including secondary variables.
```{r}
# Sensitivity, specificity and accuracy plot for logit model including secondary variables
plot(cutoffs, full_logit_list["Sensitivity", ], col = "purple", pch = 20, type = "b", lty = 2,
     ylab = "Performance", main = "Metrics for Full Logit Model")
points(cutoffs, full_logit_list["Specificity", ], col = "firebrick", pch = 20, type = "b", lty = 2)
points(cutoffs, full_logit_list["Accuracy", ], col = "chartreuse", pch = 20, type = "b", lty = 2)
abline(v = 0.35, lty = 2)
legend("bottomleft", c("Sensitivity", "Specificity", "Accuracy"), col = c("purple", "firebrick", "chartreuse"), pch = 20,
       inset = c(-0.14, -0.33), xpd = TRUE, horiz = TRUE)
```

```{r}
# Indices for cutoffs <= 0.5
cf_idx = 1:11
sens_df = data.frame(cutoff = cutoffs[cf_idx], 
           rf_model= as.numeric(forest_list["Sensitivity", cf_idx]),
           logit_model = as.numeric(logit_list["Sensitivity", cf_idx]),
           full_rf_model= as.numeric(full_forest_list["Sensitivity", cf_idx]),
           full_logit_model= as.numeric(full_logit_list["Sensitivity", cf_idx]))
# Table of sensitivities for cutoffs <= 0.5
knitr::kable(sens_df, caption = "Table I: Sensitivity across models I - IV for cutoffs <= 0.5", digits = 4)
```

```{r}
# Optimal cutoffs
opt_idx = c(7, 7, 8, 8)
metric_at_cutoff = function(metric) {
  c(as.numeric(logit_list[metric, 7]),
    as.numeric(full_forest_list[metric, 7]),
    as.numeric(logit_list[metric, 8]),
    as.numeric(full_forest_list[metric, 8]))
}
opt_df = data.frame(model = c("logit_model", "full_rf_model", "logit_model", "full_rf_model"),
        cutoff = cutoffs[opt_idx],
        sensitvity = metric_at_cutoff("Sensitivity"),
        specificity = metric_at_cutoff("Specificity"),
        accuracy = metric_at_cutoff("Accuracy"))
# Table of metrics at optimal cutoffs for Model II - III
knitr::kable(opt_df, caption = "Table II:Metrics for models II - III at cutoffs: 0.30, 0.35", digits = 4)
```

## Discussion

Because we were more interested in classifying potential heart disease cases, we wanted a model that had high sensitivity. Thus, we made the decision to sacrifice a lower specificity for a higher sensitivity. 
Among all four models, a cutoff at 0.35 (represented by the black dashed line) was satisfactory.
Between models I and II, they both had the same level of sensitivity of 0.8438, but model II had a slightly higher average and was a simpler model than a random forest model. If we include secondary variables to both models at the same cutoff, both models become more complex but do worse thus we do not consider them here. Thus model II performed best.

However, if choose a less conservative approach i.e risk having more false positives, then at a cutoff of 0.30, the two models with the highest sensitivity are model III with 0.8750 and model II with 0.8594. From Table II, models II and III had the same level of specificity of 0.6471 and model III had a negligible higher accuracy. Because both models had similar predictive performances but model II was simpler, model II is our preferred model.

Our final model uses logistic regression, model II: 
`logit_train = train(heart ~ age + sex + cp + trestbps + chol + fbs + restecg,`
`data = hd_trn, method = "glm", trControl = cv_folds)` with cutoff at 0.35.

This means that secondary variables: `thalach`, `exang`, `oldpeak` did not significantly improve the predictive performance of models I - II and the primary variables: `age`, `sex`, `cp`, `trestbps`, `chol`, `fbs`, `restecg` sufficed. Further analysis can test which specific primary variables are required to have a significantly good model. In the initial data analysis, cholesterol did not appear to have a significant correlation with heart disease while age and type of chest pain did.
The other response variable `num` can be used to specify the level of severity of the heart disease of a patient.

For more information regarding heart disease visit [cdc.gov: Heart Disease](https://www.cdc.gov/heartdisease/index.htm).

## Appendix

Primary variables

- `age`: age in years 
- `sex`: sex (1 = male; 0 = female)
- `cp`: chest pain type 
    - 1: typical angina 
    - 2: atypical angina 
    - 3: non-anginal pain 
    - 4: asymptomatic 
- `trestbps`: resting blood pressure (in mm Hg on admission to the hospital) 
- `chol`: serum cholesterol in mg/dl 
- `fbs`: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 
- `restecg`: resting electrocardiographic results 
    - 0: normal 
    - 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) 
    - 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 

Secondary variables

- `thalach`: maximum heart rate achieved 
- `exang`: exercise induced angina (1 = yes; 0 = no) 
- `oldpeak`: ST depression induced by exercise relative to rest 
- `slope`: the slope of the peak exercise ST segment 
    - 1: upsloping 
    - 2: flat 
    - 3: downsloping 
- `thal`: 3 = normal; 6 = fixed defect; 7 = reversable defect

Unconsidered variables

- `ca`: number of major vessels (0-3) colored by flourosopy 
- `location`: location observation was taken (`ch`, `cl`, `hu`, `va`)

Response variables

- `num`: angiographic disease status i.e. the number of major heart vessels with greater than 50% diameter narrowing.
    - `v0`: 0 major vessels with greater than 50% diameter narrowing.
    - `v1`: 1 major vessels with greater than 50% diameter narrowing.
    - `v2`: 2 major vessels with greater than 50% diameter narrowing. 
    - `v3`: 3 major vessels with greater than 50% diameter narrowing.
    - `v4`: 4 major vessels with greater than 50% diameter narrowing.

- `heart`: binary variable describing presence of heart disease derived from `num`
    - `no` if `num` has value `v0` (negative class)
    - `yes` otherwise (positive class)
